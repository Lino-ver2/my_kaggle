{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from mymodule import pre_train_test, base_feature, spearman_and_mse,\\\n",
    "                     load_model, ensemble, pre_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,\n",
    "               train=True,\n",
    "               drops=['protein_sequence'],\n",
    "               save=False,\n",
    "               save_name=None):\n",
    "    # featuring from mymodule.py\n",
    "    df = base_feature(df)\n",
    "    #split data for training\n",
    "    idx, x, y = pre_train_test(df, train=train,  drops=drops)\n",
    "    # saving preprocessed_data\n",
    "    if save:\n",
    "        if save_name is None: raise NameError ('(arg: save_name) is None')\n",
    "        df.to_csv('dataset/featured/' + save_name, index=False)\n",
    "\n",
    "    return  idx, x, y\n",
    "# save_name = 'base.csv'\n",
    "# idx, x, y = preprocess(df,save_name=save_name, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv('dataset/featured/base.csv')\n",
    "drops = ['protein_sequence']\n",
    "idx_train, x_train, y_train = pre_train_test(base_df, train=True, drops=drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x, y ,n_splits , params ,verbose ,save=True , file_path=None):\n",
    "\n",
    "    pred = {'val': pd.Series([None]*len(y)),\n",
    "            'pred': pd.Series([None]*len(y))}\n",
    "\n",
    "    # KFold\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=123)\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in cv.split(x, y):\n",
    "        print('-'*20, f' fold_{fold} ', '-'*20)\n",
    "\n",
    "        x_tr, y_tr, idx_tr = x_train.loc[train_idx, :],\\\n",
    "                            y_train.loc[train_idx],\\\n",
    "                            idx_train.loc[train_idx]\n",
    "        x_val, y_val, idx_val = x_train.loc[val_idx, :],\\\n",
    "                                y_train.loc[val_idx],\\\n",
    "                                idx_train.loc[val_idx]\n",
    "\n",
    "        # train model\n",
    "        model = lgb.LGBMRegressor(**params)\n",
    "        model.fit(\n",
    "            x_tr, y_tr,\n",
    "            eval_set=[(x_tr, y_tr),\n",
    "            (x_val, y_val)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=verbose)\n",
    "\n",
    "        # evaluate prediction with spreaman_correlation_coefficient\n",
    "        y_pred = model.predict(x_val)\n",
    "        corr, mse = spearman_and_mse(y_val, y_pred)\n",
    "        print('correlation: {:.5}'.format(corr,),' | mse: {:.1}'.format(mse))\n",
    "        \n",
    "        pred['val'][idx_val] = y_val\n",
    "        pred['pred'][idx_val] = y_pred\n",
    "\n",
    "        # save model to file_path\n",
    "        path = file_path + f'_fold{fold}'\n",
    "        if save:\n",
    "            with open(path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "        \n",
    "        fold += 1\n",
    "        print()\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------  fold_0  --------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=50, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=50\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's l1: 5.30831\tvalid_1's l1: 6.32239\n",
      "[2000]\ttraining's l1: 4.8933\tvalid_1's l1: 6.21642\n",
      "[3000]\ttraining's l1: 4.66114\tvalid_1's l1: 6.16864\n",
      "[4000]\ttraining's l1: 4.4927\tvalid_1's l1: 6.13405\n",
      "Early stopping, best iteration is:\n",
      "[4003]\ttraining's l1: 4.49244\tvalid_1's l1: 6.13401\n",
      "correlation: 0.99999  | mse: 8e+01\n",
      "\n",
      "--------------------  fold_1  --------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=50, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=50\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's l1: 5.24272\tvalid_1's l1: 6.46682\n",
      "[2000]\ttraining's l1: 4.82009\tvalid_1's l1: 6.37013\n",
      "[3000]\ttraining's l1: 4.5682\tvalid_1's l1: 6.32661\n",
      "Early stopping, best iteration is:\n",
      "[3380]\ttraining's l1: 4.50214\tvalid_1's l1: 6.31851\n",
      "correlation: 0.99999  | mse: 9e+01\n",
      "\n",
      "--------------------  fold_2  --------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=50, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=50\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's l1: 5.26173\tvalid_1's l1: 6.33258\n",
      "[2000]\ttraining's l1: 4.83961\tvalid_1's l1: 6.24522\n",
      "[3000]\ttraining's l1: 4.59438\tvalid_1's l1: 6.20713\n",
      "Early stopping, best iteration is:\n",
      "[3369]\ttraining's l1: 4.5285\tvalid_1's l1: 6.19633\n",
      "correlation: 0.99999  | mse: 8e+01\n",
      "\n",
      "--------------------  fold_3  --------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=50, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=50\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's l1: 5.24028\tvalid_1's l1: 6.3388\n",
      "[2000]\ttraining's l1: 4.82268\tvalid_1's l1: 6.25561\n",
      "Early stopping, best iteration is:\n",
      "[2682]\ttraining's l1: 4.65477\tvalid_1's l1: 6.23404\n",
      "correlation: 0.99999  | mse: 8e+01\n",
      "\n",
      "--------------------  fold_4  --------------------\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=50\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=50, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=50\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[1000]\ttraining's l1: 5.24504\tvalid_1's l1: 6.32234\n",
      "[2000]\ttraining's l1: 4.83365\tvalid_1's l1: 6.24245\n",
      "Early stopping, best iteration is:\n",
      "[2058]\ttraining's l1: 4.8189\tvalid_1's l1: 6.23964\n",
      "correlation: 0.99999  | mse: 8e+01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l1', \n",
    "    'metric': 'mean_absolute_error',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 32,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 50,\n",
    "    'n_estimators': 10000,\n",
    "    \"random_state\": 123,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "\n",
    "file_name = 'base_test'\n",
    "file_path = os.path.join('models', file_name)\n",
    "n_splits = 5\n",
    "verbose = 1000\n",
    "result = training(x, y ,n_splits ,params ,verbose ,True ,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "596b88989fc0dc1fed1e4e461c9c9f08188a37ef0bdca6efc263739311be1bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
